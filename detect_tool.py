"""
æ£€æµ‹å·¥å…·å‡½æ•° - å›¾åƒåˆ‡ç‰‡ã€æ‹¼æ¥ã€åæ ‡æ˜ å°„ç­‰
"""
import cv2
import numpy as np
import torch
from typing import List, Tuple, Dict
from pathlib import Path
import config


def validate_image_size(img: np.ndarray) -> Tuple[bool, str]:
    """éªŒè¯å›¾åƒå°ºå¯¸æ˜¯å¦ç¬¦åˆè¦æ±‚"""
    h, w = img.shape[:2]
    
    if h < config.MIN_IMAGE_SIZE or w < config.MIN_IMAGE_SIZE:
        return False, f"å›¾åƒå°ºå¯¸è¿‡å° ({w}x{h}), æœ€å°è¦æ±‚ {config.MIN_IMAGE_SIZE}x{config.MIN_IMAGE_SIZE}"
    
    if h > config.MAX_IMAGE_SIZE or w > config.MAX_IMAGE_SIZE:
        return False, f"å›¾åƒå°ºå¯¸è¿‡å¤§ ({w}x{h}), æœ€å¤§é™åˆ¶ {config.MAX_IMAGE_SIZE}x{config.MAX_IMAGE_SIZE}"
    
    return True, "OK"


def slice_image(img: np.ndarray, tile_size: int = 640, overlap: int = 0) -> List[Dict]:
    """
    å°†å¤§å›¾åˆ‡ç‰‡æˆå¤šä¸ªå°å—
    
    Args:
        img: åŸå§‹å›¾åƒ (H, W, C)
        tile_size: åˆ‡ç‰‡å¤§å°
        overlap: é‡å åƒç´ æ•°
        
    Returns:
        List of dict with 'image', 'x_offset', 'y_offset'
    """
    h, w = img.shape[:2]
    tiles = []
    
    stride = tile_size - overlap
    
    for y in range(0, h, stride):
        for x in range(0, w, stride):
            # è®¡ç®—åˆ‡ç‰‡åŒºåŸŸ
            x_end = min(x + tile_size, w)
            y_end = min(y + tile_size, h)
            
            # å¦‚æœåˆ‡ç‰‡å¤ªå°ï¼Œè°ƒæ•´èµ·å§‹ä½ç½®
            if x_end - x < tile_size and x > 0:
                x = max(0, x_end - tile_size)
            if y_end - y < tile_size and y > 0:
                y = max(0, y_end - tile_size)
            
            # æå–åˆ‡ç‰‡
            tile = img[y:y_end, x:x_end].copy()
            
            # å¦‚æœåˆ‡ç‰‡å°äºç›®æ ‡å°ºå¯¸ï¼Œè¿›è¡Œpadding
            if tile.shape[0] < tile_size or tile.shape[1] < tile_size:
                padded = np.zeros((tile_size, tile_size, 3), dtype=img.dtype)
                padded[:tile.shape[0], :tile.shape[1]] = tile
                tile = padded
            
            tiles.append({
                'image': tile,
                'x_offset': x,
                'y_offset': y,
                'tile_h': y_end - y,
                'tile_w': x_end - x
            })
    
    return tiles


def preprocess_image(img: np.ndarray, target_size: int = 640) -> torch.Tensor:
    """
    é¢„å¤„ç†å›¾åƒç”¨äºæ¨¡å‹æ¨ç†
    
    Args:
        img: è¾“å…¥å›¾åƒ (H, W, C) BGRæ ¼å¼
        target_size: ç›®æ ‡å°ºå¯¸
        
    Returns:
        å¤„ç†åçš„tensor (1, 3, H, W)
    """
    # ç¡®ä¿å›¾åƒæ˜¯640x640
    if img.shape[0] != target_size or img.shape[1] != target_size:
        img = cv2.resize(img, (target_size, target_size), interpolation=cv2.INTER_LINEAR)
    
    # BGR to RGB
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # å½’ä¸€åŒ–åˆ° [0, 1]
    img_float = img_rgb.astype(np.float32) / 255.0
    
    # è½¬æ¢ä¸ºtensor: (H, W, C) -> (1, C, H, W)
    img_tensor = torch.from_numpy(img_float).permute(2, 0, 1).unsqueeze(0)
    
    return img_tensor


def merge_detections(all_detections: List[Dict], img_shape: Tuple[int, int], 
                     nms_threshold: float = 0.5) -> Dict:
    """
    åˆå¹¶æ‰€æœ‰åˆ‡ç‰‡çš„æ£€æµ‹ç»“æœï¼Œå¹¶è¿›è¡ŒNMS
    
    Args:
        all_detections: æ‰€æœ‰åˆ‡ç‰‡çš„æ£€æµ‹ç»“æœ
        img_shape: åŸå›¾å°ºå¯¸ (H, W)
        nms_threshold: NMSé˜ˆå€¼
        
    Returns:
        åˆå¹¶åçš„æ£€æµ‹ç»“æœ {'boxes': [], 'scores': [], 'labels': []}
    """
    if not all_detections:
        return {'boxes': [], 'scores': [], 'labels': []}
    
    all_boxes = []
    all_scores = []
    all_labels = []
    
    # æ”¶é›†æ‰€æœ‰æ£€æµ‹æ¡†
    for det in all_detections:
        if len(det['boxes']) > 0:
            all_boxes.extend(det['boxes'])
            all_scores.extend(det['scores'])
            all_labels.extend(det['labels'])
    
    if not all_boxes:
        return {'boxes': [], 'scores': [], 'labels': []}
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    boxes = np.array(all_boxes)
    scores = np.array(all_scores)
    labels = np.array(all_labels)
    
    # è£å‰ªåˆ°å›¾åƒè¾¹ç•Œ
    boxes[:, 0] = np.clip(boxes[:, 0], 0, img_shape[1])
    boxes[:, 1] = np.clip(boxes[:, 1], 0, img_shape[0])
    boxes[:, 2] = np.clip(boxes[:, 2], 0, img_shape[1])
    boxes[:, 3] = np.clip(boxes[:, 3], 0, img_shape[0])
    
    # åˆ†ç±»åˆ«è¿›è¡ŒNMS
    final_boxes = []
    final_scores = []
    final_labels = []
    
    for cls_id in range(config.NUM_CLASSES):
        cls_mask = labels == cls_id
        if not cls_mask.any():
            continue
        
        cls_boxes = boxes[cls_mask]
        cls_scores = scores[cls_mask]
        
        # ä½¿ç”¨torchvisionçš„NMS
        boxes_tensor = torch.from_numpy(cls_boxes).float()
        scores_tensor = torch.from_numpy(cls_scores).float()
        
        from torchvision.ops import nms
        keep_idx = nms(boxes_tensor, scores_tensor, nms_threshold)
        keep_idx = keep_idx.numpy()
        
        final_boxes.extend(cls_boxes[keep_idx].tolist())
        final_scores.extend(cls_scores[keep_idx].tolist())
        final_labels.extend([cls_id] * len(keep_idx))
    
    return {
        'boxes': final_boxes,
        'scores': final_scores,
        'labels': final_labels
    }


def draw_detections(img: np.ndarray, detections: Dict, label_map: Dict = None) -> np.ndarray:
    """
    åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹æ¡†
    
    Args:
        img: åŸå§‹å›¾åƒ
        detections: æ£€æµ‹ç»“æœ
        label_map: ç±»åˆ«æ˜ å°„
        
    Returns:
        ç»˜åˆ¶åçš„å›¾åƒ
    """
    if label_map is None:
        label_map = config.LABEL_MAP
    
    img_result = img.copy()
    boxes = detections['boxes']
    scores = detections['scores']
    labels = detections['labels']
    
    colors = [(0, 255, 0), (255, 0, 0), (0, 0, 255)]
    
    for box, score, label in zip(boxes, scores, labels):
        x1, y1, x2, y2 = map(int, box)
        
        # ç»˜åˆ¶æ¡†
        color = colors[label % len(colors)]
        cv2.rectangle(img_result, (x1, y1), (x2, y2), color, 2)
        
        # ç»˜åˆ¶æ ‡ç­¾
        label_text = label_map.get(label, f'class{label}')
        label_str = f"{label_text}:{score:.2f}"
        
        (text_w, text_h), _ = cv2.getTextSize(label_str, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
        cv2.rectangle(img_result, (x1, y1 - text_h - 4), 
                     (x1 + text_w, y1), color, -1)
        cv2.putText(img_result, label_str, (x1, y1 - 2), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    
    return img_result


def detect_large_image(model, img_path: str, confidence: float = 0.1) -> Tuple[Dict, np.ndarray]:
    """
    æ£€æµ‹å¤§å›¾ - è‡ªåŠ¨åˆ‡ç‰‡ã€æ£€æµ‹ã€åˆå¹¶
    
    Args:
        model: æ£€æµ‹æ¨¡å‹
        img_path: å›¾åƒè·¯å¾„
        confidence: ç½®ä¿¡åº¦é˜ˆå€¼
        
    Returns:
        (æ£€æµ‹ç»“æœ, ç»“æœå›¾åƒ)
    """
    # è¯»å–å›¾åƒ
    img = cv2.imread(img_path)
    if img is None:
        raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {img_path}")
    
    # éªŒè¯å°ºå¯¸
    valid, msg = validate_image_size(img)
    if not valid:
        raise ValueError(msg)
    
    h, w = img.shape[:2]
    print(f"ğŸ“ åŸå›¾å°ºå¯¸: {w}x{h}")
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ‡ç‰‡
    if w <= config.IMG_SIZE and h <= config.IMG_SIZE:
        print("ğŸ“¦ å›¾åƒå°ºå¯¸å°äº640x640ï¼Œç›´æ¥æ£€æµ‹")
        tiles = [{'image': img, 'x_offset': 0, 'y_offset': 0, 
                 'tile_h': h, 'tile_w': w}]
    else:
        print(f"âœ‚ï¸  å›¾åƒè¾ƒå¤§ï¼Œåˆ‡ç‰‡å¤„ç†...")
        tiles = slice_image(img, tile_size=config.IMG_SIZE, overlap=50)
        print(f"   å…±åˆ‡åˆ†ä¸º {len(tiles)} ä¸ªåˆ‡ç‰‡")
    
    # å¯¹æ¯ä¸ªåˆ‡ç‰‡è¿›è¡Œæ£€æµ‹
    all_detections = []
    device = torch.device(config.DEVICE)
    
    for idx, tile_info in enumerate(tiles):
        tile_img = tile_info['image']
        x_offset = tile_info['x_offset']
        y_offset = tile_info['y_offset']
        
        # é¢„å¤„ç†
        img_tensor = preprocess_image(tile_img, config.IMG_SIZE).to(device)
        
        # æ¨ç†
        with torch.no_grad():
            cls_pred, reg_pred = model(img_tensor)
            preds = model.decode_predictions(
                cls_pred, reg_pred,
                conf_threshold=confidence,
                nms_threshold=config.NMS_THRESHOLD
            )
        
        # æå–æ£€æµ‹ç»“æœ
        detected_boxes = preds[0]['boxes'].cpu().numpy()
        detected_scores = preds[0]['scores'].cpu().numpy()
        detected_labels = preds[0]['labels'].cpu().numpy()
        
        if len(detected_boxes) > 0:
            print(f"   åˆ‡ç‰‡ {idx+1}/{len(tiles)}: æ£€æµ‹åˆ° {len(detected_boxes)} ä¸ªç›®æ ‡")
            
            # å°†åæ ‡æ˜ å°„å›åŸå›¾
            mapped_boxes = []
            for box in detected_boxes:
                x1, y1, x2, y2 = box
                mapped_boxes.append([
                    x1 + x_offset,
                    y1 + y_offset,
                    x2 + x_offset,
                    y2 + y_offset
                ])
            
            all_detections.append({
                'boxes': mapped_boxes,
                'scores': detected_scores.tolist(),
                'labels': detected_labels.tolist()
            })
    
    # åˆå¹¶æ‰€æœ‰æ£€æµ‹ç»“æœ
    print("ğŸ”— åˆå¹¶æ£€æµ‹ç»“æœ...")
    final_detections = merge_detections(all_detections, (h, w), config.NMS_THRESHOLD)
    
    print(f"âœ… æœ€ç»ˆæ£€æµ‹åˆ° {len(final_detections['boxes'])} ä¸ªç›®æ ‡")
    
    # ç»˜åˆ¶ç»“æœ
    result_img = draw_detections(img, final_detections)
    
    return final_detections, result_img